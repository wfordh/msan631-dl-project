{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digit Recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy import ndimage\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_ds = datasets.MNIST('../data', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "test_ds = datasets.MNIST('../data', train=False, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img, title=None):\n",
    "    plt.imshow(img, interpolation='none', cmap=\"gray\")\n",
    "    if title is not None: plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = iter(train_loader)\n",
    "x, y = next(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch shape:  torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "numpy shape:  (64, 1, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"torch shape: \", x.shape, y.shape)\n",
    "\n",
    "# first from torch to numpy\n",
    "X = x.numpy(); Y = y.numpy()\n",
    "print(\"numpy shape: \", X.shape)\n",
    "\n",
    "show(X[0][0], Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNet, self).__init__()\n",
    "        self.top_model = nn.Sequential(nn.Linear(28*28, M),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(M, M - 120),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(M - 120, M - 240),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(M - 240, 10))\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.top_model(x))\n",
    "#         # x = nn.AdaptiveAvgPool2d((1,1))(x)\n",
    "#         x = x.view(x.shape[0], -1) # flattening \n",
    "#         #x = nn.Dropout(0.2)(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         #x = nn.Dropout(0.2)(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the number of neurons in the hidden unit, inspired by advanced ML notebooks\n",
    "def get_model(M = 300):\n",
    "    net = nn.Sequential(nn.Linear(28*28, M),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(M, M - 120),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(M - 120, M - 240),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(M - 240, 10))\n",
    "    return net #.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, test_loader, num_epochs, model, optimizer):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            batch = images.shape[0] # size of the batch\n",
    "            # Convert torch tensor to Variable, change shape of the input\n",
    "            images = images.view(-1, 28*28)  #.cuda()\n",
    "            # labels = Variable(labels).cuda()\n",
    "        \n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = model(images)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            total += batch\n",
    "            sum_loss += batch * loss.data[0]\n",
    "            #if (i+1) % 100 == 0:\n",
    "             #   print ('Epoch [%d/%d], Loss: %.4f' \n",
    "              #     %(epoch+1, num_epochs, sum_loss/total))\n",
    "                \n",
    "        train_loss = sum_loss/total\n",
    "        print('Epoch [%d/%d], Loss: %.4f' %(epoch+1, num_epochs, train_loss))\n",
    "        val_acc, val_loss = model_accuracy_loss(model, test_loader)\n",
    "        print('Epoch [%d/%d], Valid Accuracy: %.4f, Valid Loss: %.4f' %(epoch+1, num_epochs, val_acc, val_loss))\n",
    "    return val_acc, val_loss, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy_loss(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    sum_loss = 0.0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28*28)  #.cuda()\n",
    "        # labels = Variable(labels)  #.cuda()\n",
    "        outputs = model(images)\n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        sum_loss += labels.size(0)*loss.data[0]\n",
    "        total += labels.size(0)\n",
    "        correct += pred.eq(labels.data).cpu().sum()\n",
    "    return 100 * correct / total, sum_loss/ total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taylorjames/anaconda/envs/DL-2018/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(8), tensor(2.3041))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = get_model()\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "model_accuracy_loss(net, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Round 4\n",
    "(Round 1, 2, and 3 were done in same cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taylorjames/anaconda/envs/DL-2018/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taylorjames/anaconda/envs/DL-2018/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Valid Accuracy: 96.0000, Valid Loss: 0.1790\n",
      "Epoch [2/10], Loss: 0.1352\n",
      "Epoch [2/10], Valid Accuracy: 96.0000, Valid Loss: 0.1877\n",
      "Epoch [3/10], Loss: 0.1279\n",
      "Epoch [3/10], Valid Accuracy: 96.0000, Valid Loss: 0.1604\n",
      "Epoch [4/10], Loss: 0.1201\n",
      "Epoch [4/10], Valid Accuracy: 96.0000, Valid Loss: 0.2368\n",
      "Epoch [5/10], Loss: 0.1100\n",
      "Epoch [5/10], Valid Accuracy: 96.0000, Valid Loss: 0.1544\n",
      "Epoch [6/10], Loss: 0.1303\n",
      "Epoch [6/10], Valid Accuracy: 96.0000, Valid Loss: 0.1949\n",
      "Epoch [7/10], Loss: 0.1231\n",
      "Epoch [7/10], Valid Accuracy: 96.0000, Valid Loss: 0.1602\n",
      "Epoch [8/10], Loss: 0.1172\n",
      "Epoch [8/10], Valid Accuracy: 96.0000, Valid Loss: 0.1852\n",
      "Epoch [9/10], Loss: 0.1115\n",
      "Epoch [9/10], Valid Accuracy: 96.0000, Valid Loss: 0.2115\n",
      "Epoch [10/10], Loss: 0.0963\n",
      "Epoch [10/10], Valid Accuracy: 96.0000, Valid Loss: 0.1728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(96), tensor(0.1728), tensor(1.00000e-02 *\n",
       "        9.6310))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_loader, test_loader, num_epochs=10, model=net, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model for later use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(m, p): \n",
    "    torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): \n",
    "    m.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(net, \"nn_digit.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mDigit-Recognizer\u001b[m\u001b[m             \u001b[34mdigit-recognizer-copy\u001b[m\u001b[m\r\n",
      "Digit_NN.ipynb               logistic_regression_torch.py\r\n",
      "README.md                    nn_digit.p\r\n",
      "dig_rec_scratch.ipynb        webcam_capture.py\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample use of model to predict on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model(net2, \"nn_digit.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net2(x[0].view(-1, 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pred = torch.max(output.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(x[0].numpy()[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
